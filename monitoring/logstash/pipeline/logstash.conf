input {
  # Backend logs
  file {
    path => "/logs/backend/*.log"
    start_position => "beginning"
    tags => ["backend"]
    codec => json
  }
  
  # AI service logs
  file {
    path => "/logs/ai-service/*.log"
    start_position => "beginning"
    tags => ["ai-service"]
    codec => json
  }
  
  # Nginx access logs
  file {
    path => "/logs/nginx/access.log"
    start_position => "beginning"
    tags => ["nginx", "access"]
    codec => plain
  }
  
  # Nginx error logs
  file {
    path => "/logs/nginx/error.log"
    start_position => "beginning"
    tags => ["nginx", "error"]
    codec => plain
  }
  
  # PostgreSQL logs
  file {
    path => "/logs/postgres/*.log"
    start_position => "beginning"
    tags => ["postgres"]
    codec => plain
  }
  
  # Redis logs
  file {
    path => "/logs/redis/*.log"
    start_position => "beginning"
    tags => ["redis"]
    codec => plain
  }
  
  # Celery logs
  file {
    path => "/logs/celery/*.log"
    start_position => "beginning"
    tags => ["celery"]
    codec => json
  }
}

filter {
  # Parse nginx access logs
  if "nginx" in [tags] and "access" in [tags] {
    grok {
      match => { 
        "message" => "%{NGINXACCESS}"
      }
    }
    
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
    
    mutate {
      convert => { "response" => "integer" }
      convert => { "bytes" => "integer" }
      convert => { "responsetime" => "float" }
    }
  }
  
  # Parse nginx error logs
  if "nginx" in [tags] and "error" in [tags] {
    grok {
      match => { 
        "message" => "(?<timestamp>%{YEAR}[./-]%{MONTHNUM}[./-]%{MONTHDAY}[- ]%{TIME}) \[%{LOGLEVEL:severity}\] %{POSINT:pid}#%{NUMBER:tid}: (\*%{NUMBER:connection_id} )?%{GREEDYDATA:message}"
      }
      overwrite => [ "message" ]
    }
    
    date {
      match => [ "timestamp", "yyyy/MM/dd HH:mm:ss" ]
    }
  }
  
  # Parse PostgreSQL logs
  if "postgres" in [tags] {
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{POSINT:pid}\] %{WORD:user}@%{WORD:database} %{WORD:severity}:  %{GREEDYDATA:message}"
      }
      overwrite => [ "message" ]
    }
    
    date {
      match => [ "timestamp", "ISO8601" ]
    }
  }
  
  # Add common fields
  mutate {
    add_field => { "environment" => "production" }
    add_field => { "application" => "cad-ai-platform" }
  }
  
  # Parse JSON logs from backend and AI service
  if "backend" in [tags] or "ai-service" in [tags] or "celery" in [tags] {
    # JSON parsing is handled by codec => json in input
    
    # Extract log level from structured logs
    if [level] {
      mutate {
        rename => { "level" => "severity" }
      }
    }
    
    # Parse error stack traces
    if [error] and [error][stack] {
      mutate {
        add_field => { "stack_trace" => "%{[error][stack]}" }
      }
    }
  }
  
  # Geoip for client IPs (nginx logs)
  if [clientip] {
    geoip {
      source => "clientip"
      target => "geoip"
    }
  }
  
  # Add severity level mapping
  translate {
    field => "severity"
    destination => "severity_code"
    dictionary => {
      "emergency" => 0
      "alert" => 1
      "critical" => 2
      "error" => 3
      "warning" => 4
      "notice" => 5
      "info" => 6
      "debug" => 7
    }
    fallback => 6
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "cad-ai-logs-%{+YYYY.MM.dd}"
    template_name => "cad-ai-logs"
    template => "/usr/share/logstash/templates/cad-ai-logs.json"
    template_overwrite => true
  }
  
  # Output to stdout for debugging (remove in production)
  stdout {
    codec => rubydebug
  }
}